{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3Y7hwwImdyQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKz1u5SdmdyT",
        "outputId": "4d918a96-3601-44e7-ad82-3347d42dd13b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  1]\n",
            " [ 1 -1]\n",
            " [-1  1]\n",
            " [-1 -1]]\n"
          ]
        }
      ],
      "source": [
        "# the 4 possible inputs for a logical operator\n",
        "X_train = np.array([[1,1],[1,-1],[-1,1],[-1,-1]])\n",
        "print(X_train)\n",
        "\n",
        "# gold classes for AND, OR et XOR\n",
        "Y_train_AND = [1,-1,-1,-1]\n",
        "Y_train_OR = [1,1,1,-1]\n",
        "Y_train_XOR = [-1,1,1,-1]\n",
        "\n",
        "# TODO\n",
        "\n",
        "# implement the forward propagation of a manually defined NN\n",
        "# for a OR, a AND, and then a XOR (hence using a hidden layer)\n",
        "\n",
        "# implement training and predication\n",
        "# with Perceptron and MLPClassifier\n",
        "\n",
        "# see fit and predict methods\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron.fit\n",
        "\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
        "\n",
        "\n",
        "\n",
        "# TODO: for each of your classifiers, print the learnt parameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# W(1) weight matrix at first hidden layer\n",
        "W_1 = np.array([[1,1], [-1,-1]]).T\n",
        "print(f\"W_1:\\n{W_1}\\n\")\n",
        "\n",
        "# b(1) bias vector at first hidden layer\n",
        "b_1 = np.array([1,1])\n",
        "print(f\"b_1: {b_1}\\n\")\n",
        "\n",
        "print(f\"X:\\n {X_train}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFU2olqy8yEv",
        "outputId": "d76417d3-140b-4407-d75e-5f821e4fb677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_1:\n",
            "[[ 1 -1]\n",
            " [ 1 -1]]\n",
            "\n",
            "b_1: [1 1]\n",
            "\n",
            "X:\n",
            " [[ 1  1]\n",
            " [ 1 -1]\n",
            " [-1  1]\n",
            " [-1 -1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwWbe8UMmdyW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5013a14-74ed-49ae-e105-8f81f75fe74d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z_1:\n",
            " [[ 3 -1]\n",
            " [ 1  1]\n",
            " [ 1  1]\n",
            " [-1  3]]\n",
            "\n",
            "A_1:\n",
            " [[ 0.99505475 -0.76159416]\n",
            " [ 0.76159416  0.76159416]\n",
            " [ 0.76159416  0.76159416]\n",
            " [-0.76159416  0.99505475]]\n"
          ]
        }
      ],
      "source": [
        "# getting z(1) pre-activations vector\n",
        "Z_1 = np.dot(X_train, W_1) + b_1\n",
        "print(f\"Z_1:\\n {Z_1}\")\n",
        "\n",
        "\n",
        "\n",
        "# defining activation function g_1 as hyperbolic tangent\n",
        "g_1 = np.tanh\n",
        "\n",
        "\n",
        "\n",
        "#applying activation func to Z(1)\n",
        "A_1 = g_1(Z_1)\n",
        "\n",
        "print(f\"\\nA_1:\\n {A_1}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining W(2) vector and b(2) bias for AND\n",
        "W_2 = np.array([[1], [1]])\n",
        "b_2 = np.array([-1])\n",
        "\n",
        "# computing A(1).W(2) + b(2)\n",
        "Z_2 = np.dot(A_1, W_2) + b_2\n",
        "\n",
        "print(f\"Z_2:\\n{Z_2}\")\n",
        "\n",
        "#get sign for each input vector\n",
        "O = np.sign(Z_2)\n",
        "print(f\"\\nOutputs: \\n{O}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lspAaCsbHjUu",
        "outputId": "85ad8e8c-239c-414e-bb90-0243ce06bc5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z_2:\n",
            "[[-0.7665394 ]\n",
            " [ 0.52318831]\n",
            " [ 0.52318831]\n",
            " [-0.7665394 ]]\n",
            "\n",
            "Outputs: \n",
            "[[-1.]\n",
            " [ 1.]\n",
            " [ 1.]\n",
            " [-1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training implementation of AND and OR operators using Perceptron class of sklearn package\n",
        "\n",
        "# AND\n",
        "perceptron = Perceptron()\n",
        "perceptron.fit(X_train, Y_train_AND)\n",
        "\n",
        "print(f\"Weights for AND: {perceptron.coef_}\")\n",
        "print(f\"Bias for AND: {perceptron.intercept_}\")\n",
        "\n",
        "print(f\"Predictions: {perceptron.predict(X_train)}\")\n",
        "print(f\"Score: {perceptron.score(X_train, Y_train_AND)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5FJ85sBzPJA",
        "outputId": "c2e26473-a57d-479f-e503-6bd4f7b9b77d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights for AND: [[1. 1.]]\n",
            "Bias for AND: [-1.]\n",
            "Predictions: [ 1 -1 -1 -1]\n",
            "Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OR\n",
        "perceptron = Perceptron()\n",
        "perceptron.fit(X_train, Y_train_OR)\n",
        "\n",
        "print(f\"Weights for OR: {perceptron.coef_}\")\n",
        "print(f\"Bias for OR: {perceptron.intercept_}\")\n",
        "\n",
        "print(f\"Predictions: {perceptron.predict(X_train)}\")\n",
        "print(f\"Score: {perceptron.score(X_train, Y_train_OR)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90mhBHU_0adA",
        "outputId": "fd1e4127-e9ae-4b47-827e-9dc99a0f03b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights for OR: [[1. 1.]]\n",
            "Bias for OR: [1.]\n",
            "Predictions: [ 1  1  1 -1]\n",
            "Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: for MLPClassifier: test with 1 hidden layer with 2 neurons, then with 100 neurons\n",
        "\n",
        "# MLP with one hidden layer of 2 neurons\n",
        "mlp = MLPClassifier(solver='lbfgs', activation='logistic', hidden_layer_sizes=(2,))\n",
        "mlp.fit(X_train, Y_train_XOR)\n",
        "\n",
        "print(f\"W(1):\\n{mlp.coefs_[0]}\")\n",
        "print(f\"\\nw(2):\\n{mlp.coefs_[1]}\")\n",
        "\n",
        "print(f\"\\nb(1): {mlp.intercepts_[0]}\")\n",
        "print(f\"\\nb(2): {mlp.intercepts_[1]}\")\n",
        "\n",
        "print(f\"Score: {mlp.score(X_train, Y_train_XOR)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDcKSmvRy0Ic",
        "outputId": "b5bb0852-f660-4d51-a9b1-1a0266590380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W(1):\n",
            "[[ 5.99530257  6.05771474]\n",
            " [-3.11903489  3.21859834]]\n",
            "\n",
            "w(2):\n",
            "[[-7.76317891]\n",
            " [ 7.73493482]]\n",
            "\n",
            "b(1): [6.58391386 6.69276514]\n",
            "\n",
            "b(2): [0.02782573]\n",
            "Score: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP with one hidden layer of 100 neurons\n",
        "mlp = MLPClassifier(solver='lbfgs', activation='logistic', hidden_layer_sizes=(100,))\n",
        "mlp.fit(X_train, Y_train_XOR)\n",
        "\n",
        "print(f\"W(1):\\n{mlp.coefs_[0]}\")\n",
        "print(f\"\\nw(2):\\n{mlp.coefs_[1]}\")\n",
        "\n",
        "print(f\"\\nb(1): {mlp.intercepts_[0]}\")\n",
        "print(f\"\\nb(2): {mlp.intercepts_[1]}\")\n",
        "\n",
        "print(f\"Score: {mlp.score(X_train, Y_train_XOR)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxxA5X2i6nHY",
        "outputId": "a341cd8d-da7c-4b0b-fed0-7d03913e7b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W(1):\n",
            "[[-2.64062568e-01  4.66747795e-01  8.56203888e-01 -8.50324514e-01\n",
            "  -2.61352009e+00  1.50815398e-01 -1.15058873e+00 -7.54556616e-01\n",
            "  -3.44339973e-01  1.34329155e-01  1.38000580e+00 -2.24915768e-01\n",
            "   2.26282859e-01 -5.81440611e-01 -1.67524792e-01  1.08509297e+00\n",
            "   4.58868797e-01  1.73018932e+00  6.37557094e-01  5.21436151e-01\n",
            "  -1.61143424e-01 -2.52272311e-01 -3.69867110e-01  2.07410453e-01\n",
            "  -3.56138398e-01 -3.28723291e-01  1.14178005e+00  2.22418042e-01\n",
            "  -2.30358983e-01  4.29528906e-02 -2.81897343e+00 -8.77933237e-02\n",
            "  -6.87805619e-01 -3.20173333e-02 -1.36560473e-01  2.38777175e-01\n",
            "   4.42414721e-01  8.98704559e-01  4.18061568e-01 -3.14844401e-01\n",
            "   1.97024981e-01  2.39500124e-03 -7.01442062e-01 -1.50857290e-01\n",
            "   1.27957067e-01  2.09037938e+00  1.48958410e-01 -2.73421390e-01\n",
            "   2.35964037e-02 -1.38218404e+00 -3.63775200e-01  3.74379245e-02\n",
            "  -2.54075794e-01  1.59651498e-01  6.31245716e-01  1.28818761e-01\n",
            "   1.18140848e+00 -6.12498090e-01  1.45478509e-01  4.37316156e-01\n",
            "  -7.36943137e-01 -2.67138582e+00 -2.55206082e-01  1.29829784e-01\n",
            "   2.81956560e-01 -6.41613165e-02  2.32275724e-01 -8.63697889e-02\n",
            "   3.96295691e-02 -1.99589153e-01 -2.25185427e-01 -3.08303949e-01\n",
            "   1.42854553e+00  7.16809102e-01  5.98482295e-01  1.58921528e+00\n",
            "   5.41454308e-01 -1.49280291e-01  2.26771598e-04  2.32746513e-01\n",
            "  -2.42377627e-01  2.27709547e-01 -7.79632038e-01  1.97315763e+00\n",
            "  -4.74923956e-01 -2.23330855e+00  3.09765096e-01  1.65212342e+00\n",
            "  -2.67938692e+00  7.09165350e-01  4.70813116e-01  6.15446877e-01\n",
            "   4.02847661e-01  3.65907796e-01  2.92203445e-01 -1.22901497e-01\n",
            "   5.89991549e-03  5.16911363e-01  6.49128831e-02  2.53899415e-01]\n",
            " [ 2.87189736e-01 -3.78076537e-01  1.16789563e+00  6.69532606e-01\n",
            "   2.19293739e+00  2.64420322e-01  1.08885216e+00  3.61662516e-01\n",
            "  -1.87294465e-01  1.45108579e-01 -1.80978735e+00  2.72482301e-02\n",
            "   2.60539116e-01  7.41218405e-01  3.46277056e-02 -7.68560899e-01\n",
            "   3.07217623e-01  2.38842960e+00 -5.28371280e-01 -8.92209329e-02\n",
            "  -5.43108845e-03 -2.51924146e-01  1.87353939e-01 -2.48745922e-01\n",
            "   1.51915055e-01  4.60347436e-01 -1.07916012e+00 -9.44797435e-02\n",
            "   3.51229448e-01 -5.61905843e-02 -2.95679400e+00 -5.76074917e-02\n",
            "  -3.80553755e-02 -1.16908105e-02  8.77421552e-02 -7.80749107e-02\n",
            "  -5.71024778e-01  1.48590609e+00 -4.17484147e-01  2.64040406e-01\n",
            "  -5.56927979e-02 -1.91269363e-01  9.73222772e-01 -8.32125757e-02\n",
            "   7.90940430e-02 -2.00004029e+00  1.38823366e-01  7.49369479e-02\n",
            "   5.66459099e-01 -1.20675031e+00 -5.27963297e-01  2.59371738e-01\n",
            "   4.01631030e-02  2.02254472e-01 -8.65973179e-01  1.52213407e-01\n",
            "  -1.05265278e+00 -3.66918058e-01 -7.07944078e-02 -2.28295714e-01\n",
            "   9.42218953e-01  2.38485558e+00  2.07383367e-01 -3.13116803e-03\n",
            "  -5.37019133e-01 -6.73448684e-03 -2.59759943e-01  2.75961147e-02\n",
            "   3.52715271e-01  2.25099500e-01  3.29642374e-01  1.63752603e-01\n",
            "  -1.09438738e+00  9.70349152e-01 -4.33991671e-01  1.63682085e+00\n",
            "   2.52758572e-01 -4.74046295e-01  5.65272978e-02  1.33625672e-01\n",
            "  -3.52010459e-01 -2.91559089e-01 -5.93513389e-01  1.84876120e+00\n",
            "   4.12194249e-01 -2.07902852e+00 -6.69919950e-01 -1.49086472e+00\n",
            "  -2.46158739e+00 -6.10885436e-01 -6.20464623e-01 -5.55665895e-01\n",
            "   3.08461242e-02  5.96848164e-01 -2.40162930e-01  1.25119006e-01\n",
            "   3.96344449e-02 -5.69770762e-01  3.70369370e-03 -2.98237424e-01]]\n",
            "\n",
            "w(2):\n",
            "[[-4.12998570e-01]\n",
            " [ 6.84815714e-01]\n",
            " [-1.21455976e+00]\n",
            " [-9.13528524e-01]\n",
            " [ 4.10505214e+00]\n",
            " [ 3.23468372e-02]\n",
            " [-1.56388986e+00]\n",
            " [-7.82531351e-01]\n",
            " [-4.02091238e-02]\n",
            " [-7.92862873e-02]\n",
            " [ 2.23312116e+00]\n",
            " [-2.38570910e-01]\n",
            " [-1.28672011e-01]\n",
            " [ 6.34355539e-01]\n",
            " [ 3.73134780e-02]\n",
            " [ 3.51470291e-01]\n",
            " [-1.09817781e-01]\n",
            " [ 3.48520296e+00]\n",
            " [ 5.70075448e-01]\n",
            " [ 3.88661265e-01]\n",
            " [-1.84465749e-01]\n",
            " [-5.47318789e-02]\n",
            " [-3.90899935e-01]\n",
            " [ 1.92213429e-01]\n",
            " [-6.15740316e-02]\n",
            " [-6.56323287e-01]\n",
            " [ 1.25455496e+00]\n",
            " [ 2.92778788e-02]\n",
            " [-3.63622950e-01]\n",
            " [ 7.88679639e-02]\n",
            " [ 4.35410894e+00]\n",
            " [-2.03107150e-01]\n",
            " [-4.17428746e-01]\n",
            " [-7.87744496e-02]\n",
            " [-2.97003784e-01]\n",
            " [-1.02744956e-01]\n",
            " [-5.31274383e-01]\n",
            " [ 8.11474255e-01]\n",
            " [ 4.15773626e-01]\n",
            " [-3.58756758e-01]\n",
            " [ 3.45328874e-02]\n",
            " [ 6.70628508e-02]\n",
            " [-1.17532282e+00]\n",
            " [-1.04847497e-01]\n",
            " [-9.41789827e-02]\n",
            " [-3.13455905e+00]\n",
            " [-1.76436923e-01]\n",
            " [-3.67497273e-01]\n",
            " [-2.59623406e-01]\n",
            " [-1.76690160e+00]\n",
            " [-2.81484703e-01]\n",
            " [-2.85427146e-01]\n",
            " [-2.52360437e-01]\n",
            " [-1.89618012e-01]\n",
            " [-1.11276963e+00]\n",
            " [ 1.24301236e-01]\n",
            " [ 1.44077763e+00]\n",
            " [-3.79329684e-01]\n",
            " [ 6.21047972e-02]\n",
            " [ 5.13728373e-01]\n",
            " [ 8.01063209e-01]\n",
            " [ 5.33738785e+00]\n",
            " [-3.91750516e-01]\n",
            " [-2.72945080e-02]\n",
            " [ 1.69262998e-01]\n",
            " [-1.07393674e-01]\n",
            " [ 1.34054472e-01]\n",
            " [-5.31907731e-02]\n",
            " [-1.58312792e-01]\n",
            " [ 2.29068930e-01]\n",
            " [-2.14499704e-01]\n",
            " [-3.47465176e-01]\n",
            " [ 1.27578348e+00]\n",
            " [-6.98871599e-01]\n",
            " [ 6.10442928e-01]\n",
            " [-2.01814671e+00]\n",
            " [ 6.50714384e-01]\n",
            " [ 4.79090022e-02]\n",
            " [-1.53288575e-01]\n",
            " [-4.23760062e-02]\n",
            " [ 7.95602356e-02]\n",
            " [-3.78916947e-01]\n",
            " [-5.87611283e-01]\n",
            " [-2.40019934e+00]\n",
            " [-5.85054580e-01]\n",
            " [ 2.71702147e+00]\n",
            " [ 4.39015887e-01]\n",
            " [ 2.30173448e+00]\n",
            " [-5.12386193e+00]\n",
            " [ 9.76634029e-01]\n",
            " [-6.21653720e-01]\n",
            " [ 7.37234423e-01]\n",
            " [ 1.76410594e-01]\n",
            " [ 2.07778511e-01]\n",
            " [ 2.56703167e-01]\n",
            " [ 2.34462421e-01]\n",
            " [-1.07288782e-01]\n",
            " [-6.78796426e-01]\n",
            " [-2.97653675e-04]\n",
            " [ 2.03738009e-01]]\n",
            "\n",
            "b(1): [ 0.45010018 -0.73394267 -1.599798    1.35455129 -2.76236227 -0.05293189\n",
            "  1.41504593  1.08972051  0.74010042 -0.81371937 -1.78993356  0.2510435\n",
            " -0.63742372 -0.98690499 -0.12845984 -0.79077433 -0.83789766  2.30697126\n",
            " -1.00115185  0.12029015  0.1755855   0.50263769  0.56136009 -0.18189543\n",
            " -0.30277602  0.68475629 -1.19159278 -0.03307155  0.46418039 -0.03813688\n",
            "  2.51997108  0.09589212  0.33881133  0.01416969  0.07171091  0.2209351\n",
            "  0.77023027  1.00323622 -0.75001338  0.51140893 -0.16951817  0.0073688\n",
            "  1.08290855  0.21776361 -0.23787688  2.47861463 -0.02413258  0.38258189\n",
            "  0.67547233 -1.47483443 -0.45033797 -0.39506394  0.3394259  -0.08574418\n",
            "  1.36159836 -0.1808814  -1.36139141 -0.31766573 -0.24779186 -0.61925693\n",
            " -1.23053723 -3.3532777   0.95761661 -0.17953174 -0.22700375 -0.00626451\n",
            " -1.01004714  0.1073374  -1.93264023 -0.44222228  0.07093002  0.4280799\n",
            " -1.66841428 -1.58522561 -0.97185547 -2.12689033  0.5739319  -0.36630625\n",
            " -0.89609505 -0.39396946  0.68275148  0.50796776 -0.34305247 -2.15029913\n",
            "  0.60528657  2.17374655 -1.37582711 -1.83253565 -2.6695935  -1.5264237\n",
            "  0.78964632 -0.98794356 -0.11944274  0.78611638 -0.25429639 -0.67528135\n",
            " -0.0603673   0.79121384 -0.10799622 -0.19907042]\n",
            "\n",
            "b(2): [-0.10961372]\n",
            "Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backpropagation example\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# define the sigmoid function\n",
        "def sigmoid(x, derivative=False):\n",
        "\n",
        "    if (derivative == True):\n",
        "        return sigmoid(x,derivative=False) * (1 - sigmoid(x,derivative=False))\n",
        "    else:\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# choose a random seed for reproducible results\n",
        "np.random.seed(1)\n",
        "\n",
        "# learning rate\n",
        "alpha = .1\n",
        "\n",
        "# number of nodes in the hidden layer\n",
        "num_hidden = 3\n",
        "\n",
        "# inputs\n",
        "X = np.array([  \n",
        "    [0, 0, 1],\n",
        "    [0, 1, 1],\n",
        "    [1, 0, 0],\n",
        "    [1, 1, 0],\n",
        "    [1, 0, 1],\n",
        "    [1, 1, 1],\n",
        "])\n",
        "\n",
        "# outputs\n",
        "# x.T is the transpose of x, making this a column vector\n",
        "y = np.array([[0, 1, 0, 1, 1, 0]]).T\n",
        "\n",
        "# initialize weights randomly with mean 0 and range [-1, 1]\n",
        "# the +1 in the 1st dimension of the weight matrices is for the bias weight\n",
        "hidden_weights = 2*np.random.random((X.shape[1] + 1, num_hidden)) - 1\n",
        "output_weights = 2*np.random.random((num_hidden + 1, y.shape[1])) - 1\n",
        "\n",
        "# number of iterations of gradient descent\n",
        "num_iterations = 10000\n",
        "\n",
        "# for each iteration of gradient descent\n",
        "for i in range(num_iterations):\n",
        "\n",
        "    # forward phase\n",
        "    # np.hstack((np.ones(...), X) adds a fixed input of 1 for the bias weight\n",
        "    input_layer_outputs = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "    hidden_layer_outputs = np.hstack((np.ones((X.shape[0], 1)), sigmoid(np.dot(input_layer_outputs, hidden_weights))))\n",
        "    output_layer_outputs = np.dot(hidden_layer_outputs, output_weights)\n",
        "\n",
        "    # backward phase\n",
        "    # output layer error term\n",
        "    output_error = output_layer_outputs - y\n",
        "    # hidden layer error term\n",
        "    # [:, 1:] removes the bias term from the backpropagation\n",
        "    hidden_error = hidden_layer_outputs[:, 1:] * (1 - hidden_layer_outputs[:, 1:]) * np.dot(output_error, output_weights.T[:, 1:])\n",
        "\n",
        "    # partial derivatives\n",
        "    hidden_pd = input_layer_outputs[:, :, np.newaxis] * hidden_error[: , np.newaxis, :]\n",
        "    output_pd = hidden_layer_outputs[:, :, np.newaxis] * output_error[:, np.newaxis, :]\n",
        "\n",
        "    # average for total gradients\n",
        "    total_hidden_gradient = np.average(hidden_pd, axis=0)\n",
        "    total_output_gradient = np.average(output_pd, axis=0)\n",
        "\n",
        "    # update weights\n",
        "    hidden_weights += - alpha * total_hidden_gradient\n",
        "    output_weights += - alpha * total_output_gradient\n",
        "\n",
        "# print the final outputs of the neural network on the inputs X\n",
        "print(\"Output After Training: \\n{}\".format(output_layer_outputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJFMc8IePPBG",
        "outputId": "5230ec44-0d2d-464c-9e9f-fc245e285398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output After Training: \n",
            "[[2.11135662e-04]\n",
            " [9.99525588e-01]\n",
            " [1.66889680e-04]\n",
            " [9.99576185e-01]\n",
            " [9.99362960e-01]\n",
            " [1.30185107e-03]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}